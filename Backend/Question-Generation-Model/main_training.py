# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bRQnQlvAhipwpQSmmjvgIFNoLKmyWhzt
"""


import pandas as pd
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from datasets import load_dataset
import numpy as np
from tqdm import tqdm

# 1. Кастомный класс для датасета
class QuestionGenerationDataset(Dataset):
    def __init__(self, contexts, questions, tokenizer, max_length=512):
        self.contexts = contexts
        self.questions = questions
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.contexts)

    def __getitem__(self, idx):
        context = str(self.contexts[idx])
        question = str(self.questions[idx])

        # Токенизация ввода
        source_encoding = self.tokenizer(
            f"генерировать вопрос: {context}",
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # Токенизация целевого текста
        target_encoding = self.tokenizer(
            question,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        return {
            "input_ids": source_encoding["input_ids"].squeeze(),
            "attention_mask": source_encoding["attention_mask"].squeeze(),
            "labels": target_encoding["input_ids"].squeeze()
        }

# 2. Функция для вычисления метрик
def compute_metrics(pred_ids, label_ids, tokenizer):
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    accuracy = np.mean([p == l for p, l in zip(pred_str, label_str)])
    return {"accuracy": accuracy}




# 3. Основной код обучения
def train_model(model_name):
    # Загрузка модели и токенизатора
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    check_gpu_available(model)

    # Загрузка датасета из Hugging Face
    dataset = load_dataset("Den4ikAI/ru_sberquad_long_answers")
    # data = dataset["train"]  # Датасет содержит только train split

    # Рассчитайте размер 10% от датасета
    dataset_size = len(dataset['train'])  # Предполагаем, что интересует тренировочный набор. Если нужен другой, замените 'train' на 'test' или 'validation'.
    subset_size = int(0.1 * dataset_size)

    # Создайте подмножество (10% от тренировочного набора)
    data = dataset['train'].shuffle(seed=42).select(range(subset_size))

    contexts = data["context"]
    questions = data["question"]

    # Разделение на тренировочную и тестовую выборки
    train_contexts, test_contexts, train_questions, test_questions = train_test_split(
        contexts, questions, test_size=0.2, random_state=42
    )

    # Создание датасетов
    train_dataset = QuestionGenerationDataset(train_contexts, train_questions, tokenizer)
    test_dataset = QuestionGenerationDataset(test_contexts, test_questions, tokenizer)

    # Создание загрузчиков данных
    batch = 4
    train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch)

    # Настройка устройства и оптимизатора
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    print(device.type)
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

    # Параметры обучения
    num_epochs = 3

    # Цикл обучения
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}"):
            optimizer.zero_grad()

            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss
            total_loss += loss.item()

            loss.backward()
            optimizer.step()

            del batch  # Добавить после обработки батча
            torch.cuda.empty_cache()  # Если используете CUDA

        avg_train_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch + 1} - Average training loss: {avg_train_loss:.4f}")

        torch.cuda.empty_cache()  # Если используете CUDA, очистить после эпохи

        # Оценка на тестовой выборке
        model.eval()
        total_test_loss = 0
        total_accuracy = 0

        with torch.no_grad():
            for batch in tqdm(test_loader, desc="Testing"):
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                total_test_loss += outputs.loss.item()

                # Генерация предсказаний
                preds = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=64
                )

                metrics = compute_metrics(preds, labels, tokenizer)
                total_accuracy += metrics["accuracy"]

        avg_test_loss = total_test_loss / len(test_loader)
        avg_test_accuracy = total_accuracy / len(test_loader)
        print(f"Test Loss: {avg_test_loss:.4f}")
        print(f"Test Accuracy: {avg_test_accuracy:.4f}")

    # Сохранение модели
    output_dir = "./ruT5-question-generator"
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"Model saved to {output_dir}")


def check_gpu_available(model):
    import tensorflow as tf
    print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
    print(f"GPU is available: {torch.cuda.is_available()}")
    print(f"Using: {next(model.parameters()).device}")
    print(torch.__version__)
    if torch.cuda.is_available():
        print(torch.cuda.current_device())  # Должно выводить индекс текущего GPU
        print(torch.cuda.device(torch.cuda.current_device()))  # Должно выводить имя текущего GPU
    else:
        exit(1)


# 4. Запуск обучения
if __name__ == "__main__":
    # Установка токенизатора глобально для метрик
    model_name = "cointegrated/rut5-small"
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    train_model(model_name)

    # Пример использования модели после обучения
    model = T5ForConditionalGeneration.from_pretrained("./ruT5-question-generator")
    model.eval()

    # Тестовый контекст
    test_context = "Москва - столица России, крупнейший город страны."
    input_text = f"генерировать вопрос: {test_context}"
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids

    with torch.no_grad():
        output = model.generate(input_ids, max_length=64)
    question = tokenizer.decode(output[0], skip_special_tokens=True)
    print(f"Сгенерированный вопрос: {question}")